{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb7c246c-7efb-43b6-a892-75f594309b8d",
   "metadata": {},
   "source": [
    "# Base model training and hyperparameter optimisation\n",
    "\n",
    "This notebook trains four base regressors and performs hyperparameter optimisation using `RandomizedSearchCV`.\n",
    "Each model includes a feature engineering step that prunes:\n",
    "- highly correlated RDKit descriptors\n",
    "- very rare and very common Morgan fingerprint bits\n",
    "\n",
    "The best hyperparameters for each model are saved to JSON for reuse in the ensemble stacking notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbff147-1c66-4998-bfa0-ed8663ca25c3",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "This notebook operates on `data/processed/train_val.csv`, created in `01_data_loading.ipynb`.\n",
    "\n",
    "The final test set (`final_test.csv`) is **not used** here. Model selection and tuning is performed on the\n",
    "train/validation pool using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918b79ae-0787-4f50-8975-b7c3514b674e",
   "metadata": {},
   "source": [
    "## Note on search settings (quick test)\n",
    "\n",
    "The cross-validation and search iterations in this notebook may be set to very small values\n",
    "(e.g. `cv=2`, `n_iter=1`) for fast pipeline testing (e.g. validating the stacking workflow).\n",
    "\n",
    "For final results, these settings should be increased and rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b127d4-5b15-4bb3-9a38-e580a884ed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUICK_TEST_RUN = False\n",
    "CV = 2 if QUICK_TEST_RUN else 3\n",
    "N_ITER = 1 if QUICK_TEST_RUN else 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8066fff-932b-4098-b763-43ab92f3d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c307aa7f-d4d2-47b2-8b69-b1bc7bffe081",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the training and validation dataset\n",
    "\n",
    "from mp.io import get_repo_root\n",
    "\n",
    "ROOT = get_repo_root()\n",
    "\n",
    "train_val_data = pd.read_csv(ROOT /\"data/processed/train_val.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c36ba3-ee6f-4372-91d9-ebd7e0662819",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "The following base regressors are trained and tuned in this notebook:\n",
    "\n",
    "- **CatBoost Regressor**\n",
    "- **XGBoost Regressor**\n",
    "- **k-Nearest Neighbours (KNN) Regressor**\n",
    "- **Feed-forward Neural Network (TensorFlow/Keras)**\n",
    "\n",
    "Each model is implemented as a **scikit-learn compatible estimator**, enabling a unified\n",
    "hyperparameter optimisation workflow using `RandomizedSearchCV`.  \n",
    "This ensures consistent cross-validation, scoring, and comparison across all model families."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3dbebb-42ad-4984-9187-dd4441a06b40",
   "metadata": {},
   "source": [
    "## Feature engineering pipeline\n",
    "\n",
    "All models share a common, custom **feature-engineering pipeline**, implemented in the\n",
    "`FeatureEngineer` class and applied identically across models.\n",
    "\n",
    "Key responsibilities of the pipeline include:\n",
    "\n",
    "- generation of chemically motivated ratio and fraction features from RDKit descriptors\n",
    "- removal of uninformative Morgan fingerprint bits using frequency-based thresholds\n",
    "- pruning of highly correlated numeric features via a **priority-based, deterministic** rule\n",
    "- learning NaN-imputation, normalisation, and encoding statistics from **training data only**\n",
    "- enforcing a consistent feature set and column order across all datasets\n",
    "\n",
    "The feature-engineering logic is **fit exclusively on training data** and then applied unchanged\n",
    "to validation and test splits, preventing information leakage during model evaluation and ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72186ebd-c1a0-4515-886d-a957dd8e43a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp.models.catboost_fe import CatBoostFEModel\n",
    "from mp.models.xgb_fe import XGBFEModel\n",
    "from mp.models.knn_fe import KNNFEModel\n",
    "from mp.models.nn_fe import NNFEModel\n",
    "from mp.models.model_optimization import run_random_search_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656dbc5b-f67f-495a-8bbd-ef302881727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "CB = CatBoostFEModel()\n",
    "XGB = XGBFEModel()\n",
    "KNN = KNNFEModel()\n",
    "NN = NNFEModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460785bd-35e0-4116-bdd3-0a5183a630c5",
   "metadata": {},
   "source": [
    "## Hyperparameter search spaces\n",
    "\n",
    "The parameter distributions defined below represent **refined search spaces**\n",
    "based on prior exploratory runs and model-specific experimentation.\n",
    "\n",
    "Rather than broad, uninformative ranges, these distributions focus on\n",
    "regions of the hyperparameter space that consistently produced strong\n",
    "cross-validation performance, allowing RandomizedSearchCV to be used\n",
    "more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95401114-3124-4edd-ad75-f9d24c3b3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint, uniform, loguniform\n",
    "\n",
    "#The following param grids are already well optimized after several iterations\n",
    "CB_cv_params = {\n",
    "    \n",
    "     # ---- CatBoost hyperparams ----\n",
    "    \"depth\": randint(6, 9),                        \n",
    "    \"learning_rate\": uniform(0.04, 0.02),          \n",
    "    \"n_estimators\": randint(1300, 1600),           \n",
    "    \"l2_leaf_reg\": uniform(14, 3),                \n",
    "    \"bagging_temperature\": uniform(0.6, 0.3),      \n",
    "    \"random_strength\": uniform(0.0, 0.6),         \n",
    "    \"rsm\": uniform(0.5, 0.25),                     \n",
    "    \"min_data_in_leaf\": randint(7, 12),            \n",
    "    \"leaf_estimation_iterations\": randint(1, 3),  \n",
    "    \"grow_policy\": [\"SymmetricTree\", \"Depthwise\", \"Lossguide\"], \n",
    "\n",
    "    # ---- FeatureEngineer thresholds ----\n",
    "    \"corr_threshold\": uniform(0.90, 0.07),        \n",
    "    \"min_fp_freq\": uniform(0.01, 0.025),           \n",
    "    \"max_fp_freq\": uniform(0.90, 0.08),           \n",
    "}\n",
    "\n",
    "XGB_cv_params = {\n",
    "\n",
    "    # ---- Core XGBoost hyperparameters ----\n",
    "    \"learning_rate\": uniform(0.05, 0.03),       \n",
    "    \"n_estimators\": randint(1900, 2100),          \n",
    "    \"max_depth\": randint(7, 9),                  \n",
    "    \"min_child_weight\": randint(8, 12),           \n",
    "\n",
    "    \"subsample\": uniform(0.8, 0.15),            \n",
    "    \"colsample_bytree\": uniform(0.75, 0.20),     \n",
    "    \"colsample_bylevel\": uniform(0.90, 0.10),     \n",
    "\n",
    "    \"reg_alpha\": uniform(0.6, 0.3),               \n",
    "    \"reg_lambda\": uniform(7.0, 3.0),              \n",
    "    \"gamma\": uniform(0.0, 0.25),                 \n",
    "\n",
    "    # ---- Fixed for speed & stability ----\n",
    "    \"tree_method\": [\"hist\"],\n",
    "    \"max_bin\": [256],\n",
    "\n",
    "    # ---- FeatureEngineer thresholds  ----\n",
    "    \"corr_threshold\": uniform(0.90, 0.06),        \n",
    "    \"min_fp_freq\": uniform(0.01, 0.02),          \n",
    "    \"max_fp_freq\": uniform(0.8, 0.15),          \n",
    "}\n",
    "\n",
    "KNN_cv_params = {\n",
    "  \n",
    "    # ---- KNN hyperparameters ----\n",
    "    \"n_neighbors\": randint(6, 9),        \n",
    "    \"weights\": [\"distance\"],              \n",
    "    \"p\": [1],                             \n",
    "    \"leaf_size\": randint(55, 71),         \n",
    "    \"algorithm\": [\"auto\"],\n",
    "\n",
    "    # ---- FeatureEngineer thresholds ----\n",
    "    \"corr_threshold\": uniform(0.9, 0.1),  \n",
    "    \"min_fp_freq\": uniform(0.01, 0.05),    \n",
    "    \"max_fp_freq\": uniform(0.75, 0.25),    \n",
    "}\n",
    "\n",
    "NN_cv_params = { \n",
    "\n",
    "    # ---- Training dynamics ----\n",
    "    \"learning_rate\": loguniform(1e-4, 5e-3),  \n",
    "    \"patience\":      randint(40, 55),\n",
    "    \"batch_size\":    randint(64, 128),\n",
    "\n",
    "    # ---- Regularisation ----\n",
    "    \"dropout\":       uniform(0.22, 0.14),        \n",
    "    \"l2_strength\":   loguniform(1.5e-5, 6.0e-5),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746e7019-5386-4069-a1d5-b0f2355e49fb",
   "metadata": {},
   "source": [
    "## Hyperparameter optimisation\n",
    "\n",
    "Each base model is tuned using `RandomizedSearchCV` with a common workflow:\n",
    "\n",
    "- A fixed holdout split (20%) is created for a quick sanity check\n",
    "- Hyperparameters are optimised using cross-validation on the remaining data\n",
    "- Mean absolute error (MAE) is used as the optimisation metric\n",
    "- The best estimator from cross-validation is evaluated once on the holdout set\n",
    "\n",
    "This approach provides a balance between robust model selection (via cross-validation)\n",
    "and a lightweight check for overfitting, while reserving a final, untouched test set\n",
    "for the ensemble evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32209192-3ef6-461f-8030-2da898ccdf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "SEARCH_CFG = {\n",
    "    \"cv\": CV,       \n",
    "    \"n_iter\": N_ITER,    \n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"CatBoost\": (CB, CB_cv_params, -1),\n",
    "    \"XGB\":      (XGB, XGB_cv_params, -1),\n",
    "    \"KNN\":      (KNN, KNN_cv_params, 1),\n",
    "    \"NN\":       (NN, NN_cv_params, 1),\n",
    "}\n",
    "\n",
    "search_results = {}\n",
    "holdout_scores = {}\n",
    "timings = {}\n",
    "\n",
    "for name, (estimator, param_dist, n_jobs) in models.items():\n",
    "    print(f\"\\nRunning RandomSearchCV for {name}...\")\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    search, holdout_mae = run_random_search_cv(\n",
    "        train_val_data,\n",
    "        \"mpC\",\n",
    "        SEARCH_CFG[\"cv\"],\n",
    "        SEARCH_CFG[\"n_iter\"],\n",
    "        estimator,\n",
    "        param_dist,\n",
    "        n_jobs,\n",
    "    )\n",
    "\n",
    "    elapsed = time.time() - start\n",
    "\n",
    "    search_results[name] = search\n",
    "    holdout_scores[name] = holdout_mae\n",
    "    timings[name] = elapsed\n",
    "\n",
    "    print(f\"{name} finished in {elapsed:.1f} s | Holdout MAE = {holdout_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605e30f-a8f5-4e60-9582-6cc7909b1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame([\n",
    "    {\n",
    "        \"model\": name,\n",
    "        \"cv_mae\": -search_results[name].best_score_,\n",
    "        \"holdout_mae\": holdout_scores[name],\n",
    "        \"seconds\": timings[name],\n",
    "    }\n",
    "    for name in models\n",
    "]).sort_values(\"holdout_mae\")\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2dbd4a-d5f4-4647-bd32-f64554180e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary.to_csv(ROOT / \"reports/model_search_summary.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042dd303-6708-4866-804e-62fe6b127cf7",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "Best hyperparameters are saved to:\n",
    "- `reports/best_params/catboost.json`\n",
    "- `reports/best_params/xgb.json`\n",
    "- `reports/best_params/knn.json`\n",
    "- `reports/best_params/nn.json`\n",
    "\n",
    "These are used directly in the ensemble stacking notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bfa9b6-5377-46cb-8b7f-0d5b0482d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp.io import save_json\n",
    "\n",
    "out_dir = ROOT / \"reports\" / \"best_params\"\n",
    "\n",
    "for name, search in search_results.items():\n",
    "    save_json(search.best_params_, out_dir / f\"{name.lower()}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f334862-45a3-43a9-aae1-0086b2ef380d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
