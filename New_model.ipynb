{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bcc9e4c-4457-4ead-ac35-0815895e8f58",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-optimize\n",
      "  Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.4.2)\n",
      "Collecting pyaml>=16.9 (from scikit-optimize)\n",
      "  Downloading pyaml-25.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-optimize) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-optimize) (1.6.1)\n",
      "Requirement already satisfied: packaging>=21.3 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-optimize) (24.2)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mattm\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
      "Downloading scikit_optimize-0.10.2-py2.py3-none-any.whl (107 kB)\n",
      "Downloading pyaml-25.7.0-py3-none-any.whl (26 kB)\n",
      "Installing collected packages: pyaml, scikit-optimize\n",
      "\n",
      "   ---------------------------------------- 2/2 [scikit-optimize]\n",
      "\n",
      "Successfully installed pyaml-25.7.0 scikit-optimize-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "65567df2-7d95-40a7-afc2-e9ea34ef9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SEED = 42\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a8066fff-932b-4098-b763-43ab92f3d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32012bcd-9fab-409b-a2b2-160e6ca4aa20",
   "metadata": {},
   "source": [
    "#### Load the three different datasets, add a new feature to specify which dataset each entry is from, and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9edb4afd-2c04-47be-90d1-d0d24e4ed823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_features.csv\").set_index(\"id\")\n",
    "cur_df = pd.read_csv(\"cur_features.csv\").set_index(\"id\")\n",
    "full_df = pd.read_csv(\"full_features.csv\").set_index(\"id\")\n",
    "test_df = pd.read_csv(\"test_features.csv\").set_index(\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "cdd98262-665a-4bf7-ac37-44754e85cf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"Source\"] = \"Kag\"\n",
    "cur_df[\"Source\"] = \"Curated\"\n",
    "full_df[\"Source\"] = \"Full\"\n",
    "test_df[\"Source\"] = \"Kag\"\n",
    "\n",
    "train_df[\"priority\"] = 0\n",
    "cur_df[\"priority\"] = 1\n",
    "full_df[\"priority\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "89c18c54-5462-4d3e-a10d-201f6ca9b205",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([train_df, cur_df, full_df], ignore_index = True)\n",
    "combined_df = combined_df.sort_values([\"SMILES\", \"priority\"])\n",
    "combined_df = combined_df.drop_duplicates(subset = \"SMILES\", keep = \"first\")\n",
    "combined_df = combined_df.drop(columns = {\"priority\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3491e797-ee39-4775-a3e4-d558c015a5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source\n",
      "Full       12796\n",
      "Kag         2662\n",
      "Curated     1407\n",
      "Name: count, dtype: int64\n",
      "(16865, 2088)\n"
     ]
    }
   ],
   "source": [
    "print(combined_df[\"Source\"].value_counts())\n",
    "print(combined_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "77b3def1-4f60-4e24-ad87-3b585f2446a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>Tm</th>\n",
       "      <th>MolWt</th>\n",
       "      <th>LogP</th>\n",
       "      <th>TPSA</th>\n",
       "      <th>HBD</th>\n",
       "      <th>HBA</th>\n",
       "      <th>RotB</th>\n",
       "      <th>RingCount</th>\n",
       "      <th>FracCSP3</th>\n",
       "      <th>...</th>\n",
       "      <th>FP_2039</th>\n",
       "      <th>FP_2040</th>\n",
       "      <th>FP_2041</th>\n",
       "      <th>FP_2042</th>\n",
       "      <th>FP_2043</th>\n",
       "      <th>FP_2044</th>\n",
       "      <th>FP_2045</th>\n",
       "      <th>FP_2046</th>\n",
       "      <th>FP_2047</th>\n",
       "      <th>Source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8682</th>\n",
       "      <td>B(C)(O)O</td>\n",
       "      <td>92.0</td>\n",
       "      <td>59.861</td>\n",
       "      <td>-0.9110</td>\n",
       "      <td>40.46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>B(C1CC1)(O)O</td>\n",
       "      <td>93.0</td>\n",
       "      <td>85.899</td>\n",
       "      <td>-0.3768</td>\n",
       "      <td>40.46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7354</th>\n",
       "      <td>B(C=CCC(C)C)(O)O</td>\n",
       "      <td>101.0</td>\n",
       "      <td>127.980</td>\n",
       "      <td>0.6007</td>\n",
       "      <td>40.46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Full</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4175</th>\n",
       "      <td>B(C=CCCC)(O)O</td>\n",
       "      <td>80.0</td>\n",
       "      <td>113.953</td>\n",
       "      <td>0.3547</td>\n",
       "      <td>40.46</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Full</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 2088 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                SMILES     Tm    MolWt    LogP   TPSA  HBD  HBA  RotB  \\\n",
       "8682          B(C)(O)O   92.0   59.861 -0.9110  40.46    2    2     0   \n",
       "8204      B(C1CC1)(O)O   93.0   85.899 -0.3768  40.46    2    2     1   \n",
       "7354  B(C=CCC(C)C)(O)O  101.0  127.980  0.6007  40.46    2    2     3   \n",
       "4175     B(C=CCCC)(O)O   80.0  113.953  0.3547  40.46    2    2     3   \n",
       "\n",
       "      RingCount  FracCSP3  ...  FP_2039  FP_2040  FP_2041  FP_2042  FP_2043  \\\n",
       "8682          0  1.000000  ...        0        0        0        0        0   \n",
       "8204          1  1.000000  ...        0        0        0        0        0   \n",
       "7354          0  0.666667  ...        0        0        0        0        0   \n",
       "4175          0  0.600000  ...        0        0        0        0        0   \n",
       "\n",
       "      FP_2044  FP_2045  FP_2046  FP_2047  Source  \n",
       "8682        0        0        0        0    Full  \n",
       "8204        0        0        0        0    Full  \n",
       "7354        0        0        0        0    Full  \n",
       "4175        0        0        0        0    Full  \n",
       "\n",
       "[4 rows x 2088 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4334e-af0c-424f-a41f-d4d07c6e1574",
   "metadata": {},
   "source": [
    "### Feature engineer class. Adds some additional engineered features from the Rdkit descriptors, drops very rare and very common FPs, and filters all features which are very highly correlated. Optional feature normalization and one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "76a4ecdb-86c2-4ecb-b8a3-f133c7bc27ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeatureEngineer:\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        corr_threshold: float = 1.00, \n",
    "        min_fp_freq: float = 0.00,\n",
    "        max_fp_freq: float = 1.00,\n",
    "        verbose: bool = True,\n",
    "        normalize: bool = False,   # whether to normalise numeric features\n",
    "        onehot_cats: bool = False, # whether to one-hot encode categoricals\n",
    "        cat_cols: list | None = None,  # which categorical columns to one-hot (None = auto-detect)\n",
    "    ):\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.min_fp_freq = min_fp_freq\n",
    "        self.max_fp_freq = max_fp_freq\n",
    "        self.verbose = verbose\n",
    "        self.normalize = normalize\n",
    "        self.onehot_cats = onehot_cats\n",
    "        self.cat_cols = cat_cols\n",
    "\n",
    "        # will be populated in fit()\n",
    "        self.fp_drop_cols_ = []\n",
    "        self.corr_drop_cols_ = []\n",
    "        self.fitted_ = False\n",
    "\n",
    "        # for normalisation\n",
    "        self.norm_cols_ = None\n",
    "        self.norm_means_ = None\n",
    "        self.norm_stds_ = None\n",
    "\n",
    "        # for one-hot encoding\n",
    "        self.cat_cols_ = []         # original categorical columns used for OHE\n",
    "        self.ohe_dummy_cols_ = []   # all dummy columns created\n",
    "        self.feature_order_ = None  # final column order after all FE\n",
    "\n",
    "        # for NaN filling (numeric cols)\n",
    "        self.fill_values_ = None    # pandas Series of means (index = column names)\n",
    "\n",
    "    # ---------- 0. Add engineered descriptor features ---------- #\n",
    "    @staticmethod\n",
    "    def _add_engineered_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        eps = 1e-6  # avoid div-by-zero\n",
    "\n",
    "        # H-bonding / polarity related\n",
    "        df[\"HBD_HBA_sum\"]   = df[\"HBD\"] + df[\"HBA\"]\n",
    "        df[\"HBD_HBA_ratio\"] = df[\"HBD\"] / (df[\"HBA\"] + eps)\n",
    "\n",
    "        df[\"HBD_per_heavy\"] = df[\"HBD\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "        df[\"HBA_per_heavy\"] = df[\"HBA\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "\n",
    "        df[\"TPSA_per_MW\"]     = df[\"TPSA\"] / (df[\"MolWt\"] + eps)\n",
    "        df[\"TPSA_per_heavy\"]  = df[\"TPSA\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "\n",
    "        # Heteroatom & aromaticity fractions\n",
    "        df[\"HetFrac\"]          = df[\"NumHeteroatoms\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "        df[\"AromaticFrac\"]     = df[\"NumAromaticAtoms\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "        df[\"AromaticRingFrac\"] = df[\"NumAromaticRings\"] / (df[\"RingCount\"] + eps)\n",
    "        df[\"RingDensity\"]      = df[\"RingCount\"] / (df[\"HeavyAtomCount\"] + eps)\n",
    "\n",
    "        # Packing / density-ish proxies\n",
    "        df[\"MW_per_ASA\"]     = df[\"MolWt\"] / (df[\"LabuteASA\"] + eps)\n",
    "        df[\"Heavy_per_ASA\"]  = df[\"HeavyAtomCount\"] / (df[\"LabuteASA\"] + eps)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # ---------- 1. “GET” methods: compute columns to drop ---------- #\n",
    "    def get_fp_drop_cols(self, df: pd.DataFrame):\n",
    "        fp_cols = [c for c in df.columns if c.startswith(\"FP\")]\n",
    "        if not fp_cols:\n",
    "            if self.verbose:\n",
    "                print(\"No FP columns found for fingerprint frequency filter.\")\n",
    "            return []\n",
    "\n",
    "        freq_fp = df[fp_cols].mean(axis=0)\n",
    "        below_mask = freq_fp < self.min_fp_freq\n",
    "        above_mask = freq_fp > self.max_fp_freq\n",
    "        drop_fp_cols = freq_fp[below_mask | above_mask].index.tolist()\n",
    "\n",
    "        if self.verbose:\n",
    "            before = len(fp_cols)\n",
    "            after = before - len(drop_fp_cols)\n",
    "            print(\"Fingerprint frequency filter:\")\n",
    "            print(f\" - threshold min={self.min_fp_freq}, max={self.max_fp_freq}\")\n",
    "            print(f\" - would drop {len(drop_fp_cols)} FP columns\")\n",
    "            print(f\" - remaining FP columns: {after}/{before}\")\n",
    "\n",
    "        return drop_fp_cols\n",
    "\n",
    "    \n",
    "    def get_corr_drop_cols(self, df: pd.DataFrame, target_col: str = \"Tm\"):\n",
    "        # compute corr on numeric columns only\n",
    "        corr = df.corr(numeric_only=True)\n",
    "\n",
    "        # only features (exclude target from being dropped)\n",
    "        feature_cols = [c for c in corr.columns if c != target_col]\n",
    "        if len(feature_cols) < 2:\n",
    "            if self.verbose:\n",
    "                print(\"Not enough feature columns for correlation filtering.\")\n",
    "            return []\n",
    "\n",
    "        corr_sub = corr.loc[feature_cols, feature_cols]\n",
    "\n",
    "        mask = np.triu(np.ones(corr_sub.shape, dtype=bool), k=1)\n",
    "        stacked = corr_sub.where(mask).stack()\n",
    "\n",
    "        filtered = stacked[abs(stacked) > self.corr_threshold]\n",
    "        if filtered.empty:\n",
    "            if self.verbose:\n",
    "                print(f\"No redundant pairs found at threshold {self.corr_threshold}.\")\n",
    "            return []\n",
    "\n",
    "        redundant_pairs_df = filtered.reset_index()\n",
    "        redundant_pairs_df.columns = [\"Item_1\", \"Item_2\", \"Item_1_Item_2_corr\"]\n",
    "\n",
    "        # add target correlations if available\n",
    "        if target_col in corr.index:\n",
    "            redundant_pairs_df[\"Item_1_target_cor\"] = redundant_pairs_df[\"Item_1\"].map(corr[target_col])\n",
    "            redundant_pairs_df[\"Item_2_target_cor\"] = redundant_pairs_df[\"Item_2\"].map(corr[target_col])\n",
    "        else:\n",
    "            redundant_pairs_df[\"Item_1_target_cor\"] = 0.0\n",
    "            redundant_pairs_df[\"Item_2_target_cor\"] = 0.0\n",
    "\n",
    "        item_count = pd.concat(\n",
    "            [redundant_pairs_df[\"Item_1\"], redundant_pairs_df[\"Item_2\"]],\n",
    "            ignore_index=True\n",
    "        ).value_counts()\n",
    "\n",
    "        def choose_worse(row):\n",
    "            i = row[\"Item_1\"]\n",
    "            j = row[\"Item_2\"]\n",
    "\n",
    "            # Rule A – appears in more redundant pairs\n",
    "            if item_count[i] > item_count[j]:\n",
    "                return i\n",
    "            if item_count[j] > item_count[i]:\n",
    "                return j\n",
    "\n",
    "            # Rule B – less correlated with target\n",
    "            if abs(row[\"Item_1_target_cor\"]) < abs(row[\"Item_2_target_cor\"]):\n",
    "                return i\n",
    "            else:\n",
    "                return j\n",
    "\n",
    "        redundant_pairs_df[\"drop\"] = redundant_pairs_df.apply(choose_worse, axis=1)\n",
    "        drop_cols = redundant_pairs_df[\"drop\"].unique().tolist()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\n",
    "                f\"Correlation filter: would drop {len(drop_cols)} columns at threshold {self.corr_threshold}\"\n",
    "            )\n",
    "\n",
    "        return drop_cols\n",
    "\n",
    "    # ---------- 2. Apply drops to any dataframe ---------- #\n",
    "    @staticmethod\n",
    "    def drop_columns(df: pd.DataFrame, cols_to_drop) -> pd.DataFrame:\n",
    "        return df.drop(columns=list(cols_to_drop), errors=\"ignore\")\n",
    "\n",
    "    # ---------- 3. High-level fit / transform ---------- #\n",
    "    def fit(self, df: pd.DataFrame, target_col: str = \"Tm\"):\n",
    "        \"\"\"\n",
    "        Decide which columns to drop based on TRAINING data only.\n",
    "        Does NOT modify df in-place; just stores the lists.\n",
    "        Also learns normalisation stats, NaN fill values, and one-hot encoding structure (if enabled).\n",
    "        \"\"\"\n",
    "        # 0) add engineered features first\n",
    "        df_eng = self._add_engineered_features(df)\n",
    "\n",
    "        total_cols_init = df_eng.shape[1]\n",
    "\n",
    "        # 1) frequency filters first (on engineered dataframe)\n",
    "        fp_drop = self.get_fp_drop_cols(df_eng)\n",
    "        df_tmp = self.drop_columns(df_eng, fp_drop)\n",
    "\n",
    "        # 2) correlation filter on remaining features\n",
    "        corr_drop = self.get_corr_drop_cols(df_tmp, target_col=target_col)\n",
    "\n",
    "        self.fp_drop_cols_ = fp_drop\n",
    "        self.corr_drop_cols_ = corr_drop\n",
    "        self.fitted_ = True\n",
    "\n",
    "        if self.verbose:\n",
    "            total = len(set(fp_drop) | set(corr_drop))\n",
    "            print(f\"Total unique cols to drop after fit: {total}\")\n",
    "            print(f\"Total cols remaining after drops: {(total_cols_init - total)}\")\n",
    "\n",
    "        # df after all drops (but BEFORE any one-hot) – used for norm, NaN fill, and ohe logic\n",
    "        all_drops = set(fp_drop) | set(corr_drop)\n",
    "        if \"SMILES\" in df_eng.columns:\n",
    "            all_drops.add(\"SMILES\")\n",
    "        df_after_drop = self.drop_columns(df_eng, all_drops)\n",
    "\n",
    "        # 3) Identify numeric columns with NaNs and compute fill values (means)\n",
    "        self.fill_values_ = df_after_drop.mean(numeric_only=True)\n",
    "\n",
    "        if self.verbose:\n",
    "            na_cols = df_after_drop.columns[df_after_drop.isna().any()].tolist()\n",
    "            print(f\"{len(na_cols)} cols with NaNs after drop: {na_cols}\")\n",
    "\n",
    "        # For normalisation, it's cleaner to work on a filled copy\n",
    "        df_for_norm = df_after_drop.copy()\n",
    "        if self.fill_values_ is not None and not self.fill_values_.empty:\n",
    "            intersect = [c for c in df_for_norm.columns if c in self.fill_values_.index]\n",
    "            df_for_norm[intersect] = df_for_norm[intersect].fillna(self.fill_values_[intersect])\n",
    "\n",
    "        # 4) compute normalisation stats on remaining numeric, non-FP features\n",
    "        if self.normalize:\n",
    "            num_cols = []\n",
    "            for c in df_for_norm.columns:\n",
    "                if c == target_col:\n",
    "                    continue\n",
    "                if c.startswith(\"FP\"):\n",
    "                    continue\n",
    "                dtype = df_for_norm[c].dtype\n",
    "                if dtype == \"object\" or str(dtype).startswith(\"category\"):\n",
    "                    continue\n",
    "                num_cols.append(c)\n",
    "\n",
    "            self.norm_cols_ = num_cols\n",
    "            if num_cols:\n",
    "                means = df_for_norm[num_cols].mean()\n",
    "                stds = df_for_norm[num_cols].std().replace(0.0, 1.0)\n",
    "                self.norm_means_ = means\n",
    "                self.norm_stds_ = stds\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f\"Normalisation enabled for {len(num_cols)} columns.\")\n",
    "            else:\n",
    "                if self.verbose:\n",
    "                    print(\"Normalisation enabled but no numeric columns found for scaling.\")\n",
    "\n",
    "        # 5) learn one-hot encoding structure (if enabled)\n",
    "        if self.onehot_cats:\n",
    "            # decide which categorical columns to encode\n",
    "            if self.cat_cols is not None:\n",
    "                cat_cols = [c for c in self.cat_cols if c in df_after_drop.columns]\n",
    "            else:\n",
    "                # auto-detect object/category cols (excluding target if present)\n",
    "                cat_cols = [\n",
    "                    c for c in df_after_drop.columns\n",
    "                    if c != target_col\n",
    "                    and (df_after_drop[c].dtype == \"object\"\n",
    "                         or str(df_after_drop[c].dtype).startswith(\"category\"))\n",
    "                ]\n",
    "\n",
    "            self.cat_cols_ = cat_cols\n",
    "\n",
    "            if cat_cols:\n",
    "                # one-hot on a copy so we can capture the full dummy column set\n",
    "                df_ohe = pd.get_dummies(df_after_drop, columns=cat_cols, drop_first=False)\n",
    "\n",
    "                # dummy columns are the new columns introduced\n",
    "                original_cols = set(df_after_drop.columns)\n",
    "                self.ohe_dummy_cols_ = [c for c in df_ohe.columns if c not in original_cols]\n",
    "\n",
    "                # final feature order (without target if it’s still present)\n",
    "                self.feature_order_ = [c for c in df_ohe.columns if c != target_col]\n",
    "\n",
    "                if self.verbose:\n",
    "                    print(f\"One-hot encoding enabled for {len(cat_cols)} columns.\")\n",
    "                    print(f\"Created {len(self.ohe_dummy_cols_)} dummy columns.\")\n",
    "            else:\n",
    "                # no categoricals found\n",
    "                self.ohe_dummy_cols_ = []\n",
    "                # still set feature_order_ to the columns after drop\n",
    "                self.feature_order_ = [c for c in df_after_drop.columns if c != target_col]\n",
    "                if self.verbose:\n",
    "                    print(\"One-hot encoding enabled but no categorical columns found.\")\n",
    "        else:\n",
    "            # no one-hot encoding; keep order after drops\n",
    "            self.cat_cols_ = []\n",
    "            self.ohe_dummy_cols_ = []\n",
    "            self.feature_order_ = [c for c in df_after_drop.columns if c != target_col]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Apply the learned feature engineering, drops, NaN filling,\n",
    "        (optionally) normalisation and (optionally) one-hot encoding\n",
    "        to ANY dataset (train/test/val).\n",
    "        Output columns are always in the same order as learned in fit().\n",
    "        \"\"\"\n",
    "        if not self.fitted_:\n",
    "            raise RuntimeError(\"FeatureEngineer must be fitted before calling transform().\")\n",
    "\n",
    "        # 0) add engineered features\n",
    "        df_eng = self._add_engineered_features(df)\n",
    "\n",
    "        # 1) drop learned columns\n",
    "        all_drops = set(self.fp_drop_cols_) | set(self.corr_drop_cols_)\n",
    "        if \"SMILES\" in df_eng.columns:\n",
    "            all_drops.add(\"SMILES\")\n",
    "\n",
    "        df_clean = self.drop_columns(df_eng, all_drops)\n",
    "\n",
    "        # 2) fill NaNs using training means (for numeric columns we learned means for)\n",
    "        if self.fill_values_ is not None and not self.fill_values_.empty:\n",
    "            intersect = [c for c in df_clean.columns if c in self.fill_values_.index]\n",
    "            if intersect:\n",
    "                df_clean[intersect] = df_clean[intersect].fillna(self.fill_values_[intersect])\n",
    "\n",
    "        # 3) apply normalisation if requested (only on norm_cols_)\n",
    "        if self.normalize and self.norm_cols_ is not None and self.norm_means_ is not None:\n",
    "            for c in self.norm_cols_:\n",
    "                if c in df_clean.columns:\n",
    "                    df_clean[c] = (df_clean[c] - self.norm_means_[c]) / self.norm_stds_[c]\n",
    "\n",
    "        # 4) apply one-hot encoding if enabled\n",
    "        if self.onehot_cats and self.cat_cols_:\n",
    "            # important: same categorical columns as in fit\n",
    "            df_ohe = pd.get_dummies(df_clean, columns=self.cat_cols_, drop_first=False)\n",
    "\n",
    "            # ensure all dummy columns seen during fit exist now\n",
    "            for col in self.ohe_dummy_cols_:\n",
    "                if col not in df_ohe.columns:\n",
    "                    df_ohe[col] = 0\n",
    "\n",
    "            # final reordering: use feature_order_ (no target)\n",
    "            df_ohe = df_ohe.reindex(columns=self.feature_order_, fill_value=0)\n",
    "            return df_ohe\n",
    "\n",
    "        else:\n",
    "            # no OHE: just reorder to feature_order_ (if present)\n",
    "            if self.feature_order_ is not None:\n",
    "                df_clean = df_clean.reindex(columns=self.feature_order_, fill_value=0)\n",
    "            return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "fe475a80-28ad-4c03-baf2-684f3e36b2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "class CatBoostFEModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # CatBoost params (tunable)\n",
    "        depth: int = 3,\n",
    "        learning_rate: float = 0.06,\n",
    "        n_estimators: int = 1100,\n",
    "        l2_leaf_reg: float = 2.5,\n",
    "        bagging_temperature: float = 0.25,\n",
    "        random_strength: int = 1,\n",
    "        grow_policy: str = \"SymmetricTree\",\n",
    "        rsm: float = 0.75,\n",
    "        min_data_in_leaf: int = 1,\n",
    "        leaf_estimation_iterations: int = 1,\n",
    "        loss_function: str = \"MAE\",\n",
    "        random_state: int = 42,\n",
    "        cb_verbose: bool = False,\n",
    "\n",
    "        # FeatureEngineer hyperparams (tunable)\n",
    "        corr_threshold: float = 0.90,\n",
    "        min_fp_freq: float = 0.01,\n",
    "        max_fp_freq: float = 0.80,\n",
    "        fe_verbose: bool = False,\n",
    "        target_col: str = \"Tm\",\n",
    "    ):\n",
    "        # CatBoost params\n",
    "        self.depth = depth\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.l2_leaf_reg = l2_leaf_reg\n",
    "        self.bagging_temperature = bagging_temperature\n",
    "        self.random_strength = random_strength\n",
    "        self.grow_policy = grow_policy\n",
    "        self.rsm = rsm\n",
    "        self.min_data_in_leaf = min_data_in_leaf\n",
    "        self.leaf_estimation_iterations = leaf_estimation_iterations\n",
    "        self.loss_function = loss_function\n",
    "        self.random_state = random_state\n",
    "        self.cb_verbose = cb_verbose\n",
    "\n",
    "        # FE params\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.min_fp_freq = min_fp_freq\n",
    "        self.max_fp_freq = max_fp_freq\n",
    "        self.fe_verbose = fe_verbose\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # will be created in fit\n",
    "        self.fe_ = None\n",
    "        self.model_ = None\n",
    "        self.cat_feature_indices_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: DataFrame with raw features (no Tm column)\n",
    "        y: target Series/array (Tm values)\n",
    "        \"\"\"\n",
    "        # 1. build training df with target for correlation-based FE\n",
    "        df_with_target = X.copy()\n",
    "        df_with_target[self.target_col] = y\n",
    "\n",
    "        # 2. set up FeatureEngineer with current hyperparams\n",
    "        self.fe_ = FeatureEngineer(\n",
    "            corr_threshold=self.corr_threshold,\n",
    "            min_fp_freq=self.min_fp_freq,\n",
    "            max_fp_freq=self.max_fp_freq,\n",
    "            verbose=self.fe_verbose,\n",
    "            normalize = False\n",
    "        )\n",
    "\n",
    "        # 3. fit FE and transform training data\n",
    "        self.fe_.fit(df_with_target, target_col=self.target_col)\n",
    "        X_trans = self.fe_.transform(X)\n",
    "\n",
    "        # ---- detect categorical columns (like \"Source\") ---- #\n",
    "        cat_cols = [\n",
    "            c for c in X_trans.columns\n",
    "            if X_trans[c].dtype == \"object\"\n",
    "            or str(X_trans[c].dtype).startswith(\"category\")\n",
    "        ]\n",
    "        self.cat_feature_indices_ = [\n",
    "            X_trans.columns.get_loc(c) for c in cat_cols\n",
    "        ]\n",
    "\n",
    "        if self.cb_verbose:\n",
    "            print(\"Categorical columns:\", cat_cols)\n",
    "            print(\"Categorical feature indices:\", self.cat_feature_indices_)\n",
    "\n",
    "        # 4. build CatBoost params\n",
    "        cb_params = dict(\n",
    "            random_seed=self.random_state,\n",
    "            loss_function=self.loss_function,\n",
    "            depth=self.depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            min_data_in_leaf=self.min_data_in_leaf,\n",
    "            leaf_estimation_iterations=self.leaf_estimation_iterations,\n",
    "            n_estimators=self.n_estimators,\n",
    "            l2_leaf_reg=self.l2_leaf_reg,\n",
    "            random_strength=self.random_strength,\n",
    "            bootstrap_type=\"Bayesian\",\n",
    "            bagging_temperature=self.bagging_temperature,\n",
    "            grow_policy=self.grow_policy,\n",
    "            rsm=self.rsm,\n",
    "            verbose=self.cb_verbose,\n",
    "        )\n",
    "\n",
    "        # 5. train CatBoost, explicitly passing cat_features\n",
    "        self.model_ = CatBoostRegressor(**cb_params)\n",
    "        self.model_.fit(\n",
    "            X_trans,\n",
    "            y,\n",
    "            cat_features=self.cat_feature_indices_,\n",
    "            verbose=self.cb_verbose,\n",
    "        )\n",
    "\n",
    "        train_pred = self.model_.predict(X_trans)\n",
    "        train_mae = mean_absolute_error(y, train_pred)\n",
    "        #print(f\"Training MAE: {train_mae:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply the same feature engineering and CatBoost model to new data.\n",
    "        \"\"\"\n",
    "        if self.fe_ is None or self.model_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted. Call fit(X, y) first.\")\n",
    "\n",
    "        X_trans = self.fe_.transform(X)\n",
    "        # CatBoost already knows which features are categorical from training,\n",
    "        # so we can just call predict:\n",
    "        return self.model_.predict(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "16cbe231-00ac-4ce8-82f8-ef8f2c415957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "class XGBModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        learning_rate: float = 0.07,\n",
    "        n_estimators: int = 1000,\n",
    "        max_depth: int = 6,\n",
    "        min_child_weight: int = 2,\n",
    "        subsample: float = 0.8,\n",
    "        colsample_bytree: float = 0.8,\n",
    "        reg_lambda: float = 1,\n",
    "        reg_alpha: float = 0,\n",
    "        gamma: float = 0,\n",
    "        colsample_bylevel: float = 1.0,\n",
    "        tree_method: str = \"hist\",\n",
    "        max_bin: int = 256,\n",
    "        objective: str = \"reg:absoluteerror\",\n",
    "        eval_metric: str = \"mae\",\n",
    "        random_seed: int = 42,\n",
    "        xgb_verbose: bool = False,\n",
    "\n",
    "        # FeatureEngineer hyperparams (tunable)\n",
    "        corr_threshold: float = 0.90,\n",
    "        min_fp_freq: float = 0.01,\n",
    "        max_fp_freq: float = 0.80,\n",
    "        fe_verbose: bool = False,\n",
    "        target_col: str = \"Tm\",\n",
    "    ):\n",
    "        # XGB params\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_child_weight = min_child_weight\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.gamma = gamma\n",
    "        self.colsample_bylevel = colsample_bylevel\n",
    "        self.tree_method = tree_method\n",
    "        self.max_bin = max_bin\n",
    "        self.objective = objective\n",
    "        self.eval_metric = eval_metric\n",
    "        self.random_seed = random_seed\n",
    "        self.xgb_verbose = xgb_verbose\n",
    "\n",
    "        # FeatureEngineer params\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.min_fp_freq = min_fp_freq\n",
    "        self.max_fp_freq = max_fp_freq\n",
    "        self.fe_verbose = fe_verbose\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # runtime attributes\n",
    "        self.fe_ = None\n",
    "        self.model_ = None\n",
    "        self.cat_feature_indices_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # 1. build training df with target for correlation-based FE\n",
    "        df_with_target = X.copy()\n",
    "        df_with_target[self.target_col] = y\n",
    "\n",
    "        # 2. FeatureEngineer\n",
    "        self.fe_ = FeatureEngineer(\n",
    "            corr_threshold=self.corr_threshold,\n",
    "            min_fp_freq=self.min_fp_freq,\n",
    "            max_fp_freq=self.max_fp_freq,\n",
    "            verbose=self.fe_verbose,\n",
    "            normalize=False,\n",
    "        )\n",
    "\n",
    "        self.fe_.fit(df_with_target, target_col=self.target_col)\n",
    "        X_trans = self.fe_.transform(X)\n",
    "\n",
    "        # detect categoricals\n",
    "        cat_cols = [\n",
    "            c for c in X_trans.columns\n",
    "            if X_trans[c].dtype == \"object\"\n",
    "            or str(X_trans[c].dtype).startswith(\"category\")\n",
    "        ]\n",
    "        self.cat_feature_indices_ = [\n",
    "            X_trans.columns.get_loc(c) for c in cat_cols\n",
    "        ]\n",
    "\n",
    "        if self.xgb_verbose:\n",
    "            print(\"Categorical columns:\", cat_cols)\n",
    "            print(\"Categorical feature indices:\", self.cat_feature_indices_)\n",
    "\n",
    "        if cat_cols:\n",
    "            X_trans[cat_cols] = X_trans[cat_cols].astype(\"category\")\n",
    "\n",
    "        xgb_params = dict(\n",
    "            learning_rate=self.learning_rate,\n",
    "            n_estimators=self.n_estimators,\n",
    "            max_depth=self.max_depth,\n",
    "            min_child_weight=self.min_child_weight,\n",
    "            subsample=self.subsample,\n",
    "            colsample_bytree=self.colsample_bytree,\n",
    "            colsample_bylevel=self.colsample_bylevel,\n",
    "            reg_lambda=self.reg_lambda,\n",
    "            reg_alpha=self.reg_alpha,\n",
    "            gamma=self.gamma,\n",
    "            tree_method=self.tree_method,\n",
    "            max_bin=self.max_bin,\n",
    "            objective=self.objective,\n",
    "            eval_metric=self.eval_metric,\n",
    "            random_state=self.random_seed,\n",
    "            verbosity=int(self.xgb_verbose),\n",
    "            n_jobs=-1,\n",
    "            enable_categorical=True,\n",
    "        )\n",
    "\n",
    "        self.model_ = XGBRegressor(**xgb_params)\n",
    "        self.model_.fit(X_trans, y)\n",
    "\n",
    "        train_pred = self.model_.predict(X_trans)\n",
    "        train_mae = mean_absolute_error(y, train_pred)\n",
    "        if self.xgb_verbose:\n",
    "            print(f\"Training MAE: {train_mae:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fe_ is None or self.model_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted. Call fit(X, y) first.\")\n",
    "\n",
    "        X_trans = self.fe_.transform(X)\n",
    "\n",
    "        if self.cat_feature_indices_ is not None:\n",
    "            cat_cols = [X_trans.columns[i] for i in self.cat_feature_indices_]\n",
    "            if cat_cols:\n",
    "                X_trans[cat_cols] = X_trans[cat_cols].astype(\"category\")\n",
    "\n",
    "        return self.model_.predict(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d2672484-5578-493e-b01e-04b3b951a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "class KNNModel(BaseEstimator, RegressorMixin):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_neighbors: int = 5,\n",
    "        weights: str = \"uniform\",\n",
    "        p: int = 2,\n",
    "        algorithm: str = \"auto\",\n",
    "        leaf_size: int = 30,\n",
    "        n_jobs: int = -1,\n",
    "        random_seed: int = 42,\n",
    "        knn_verbose: bool = False,\n",
    "\n",
    "        # FeatureEngineer hyperparams (tunable)\n",
    "        corr_threshold: float = 0.90,\n",
    "        min_fp_freq: float = 0.01,\n",
    "        max_fp_freq: float = 0.80,\n",
    "        fe_verbose: bool = False,\n",
    "        target_col: str = \"Tm\",\n",
    "    ):\n",
    "\n",
    "        # --- KNN hyperparams ---\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.weights = weights\n",
    "        self.p = p\n",
    "        self.algorithm = algorithm\n",
    "        self.leaf_size = leaf_size\n",
    "        self.n_jobs = n_jobs\n",
    "        self.random_seed = random_seed\n",
    "        self.knn_verbose = knn_verbose\n",
    "\n",
    "        # --- FeatureEngineer hyperparams ---\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.min_fp_freq = min_fp_freq\n",
    "        self.max_fp_freq = max_fp_freq\n",
    "        self.fe_verbose = fe_verbose\n",
    "        self.target_col = target_col\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: DataFrame with raw features (no Tm column)\n",
    "        y: target Series/array (Tm values)\n",
    "        \"\"\"\n",
    "        # 1. build training df with target for correlation-based FE\n",
    "        df_with_target = X.copy()\n",
    "        df_with_target[self.target_col] = y\n",
    "    \n",
    "        # 2. FeatureEngineer with normalization enabled for KNN\n",
    "        self.fe_ = FeatureEngineer(\n",
    "            corr_threshold=self.corr_threshold,\n",
    "            min_fp_freq=self.min_fp_freq,\n",
    "            max_fp_freq=self.max_fp_freq,\n",
    "            verbose=self.fe_verbose,\n",
    "            normalize=True,\n",
    "            onehot_cats=True, \n",
    "            cat_cols=[\"Source\"],\n",
    "        )\n",
    "    \n",
    "        # 3. fit FE and transform training data\n",
    "        self.fe_.fit(df_with_target, target_col=self.target_col)\n",
    "        X_trans = self.fe_.transform(X)\n",
    "    \n",
    "        if self.knn_verbose:\n",
    "            print(\"KNN: transformed feature shape:\", X_trans.shape)\n",
    "            print(\"KNN: dtypes after FE:\", X_trans.dtypes.value_counts())\n",
    "    \n",
    "      \n",
    "        '''\n",
    "        # 3a. drop any non-numeric (safety, e.g. if Source is still there)\n",
    "        non_numeric = X_trans.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "        if len(non_numeric) > 0 and self.knn_verbose:\n",
    "            print(\"Dropping non-numeric columns for KNN during fit:\", non_numeric.tolist())\n",
    "        X_trans = X_trans.drop(columns=non_numeric)\n",
    "    \n",
    "        # 4. NaN diagnostics + imputation\n",
    "        na_counts = X_trans.isna().sum()\n",
    "        if na_counts.any() and self.knn_verbose:\n",
    "            print(\"NaNs detected in KNN input during fit:\")\n",
    "            print(na_counts[na_counts > 0].sort_values(ascending=False).head(30))\n",
    "            print(\"Imputing NaNs with column means (then 0 as fallback).\")\n",
    "    \n",
    "        col_means = X_trans.mean(numeric_only=True)\n",
    "        X_trans = X_trans.fillna(col_means)\n",
    "        X_trans = X_trans.fillna(0.0)\n",
    "        '''\n",
    "    \n",
    "        # 5. store feature names used for training\n",
    "        self.feature_names_ = X_trans.columns.tolist()\n",
    "    \n",
    "        # 6. build and fit KNN model\n",
    "        self.model_ = KNeighborsRegressor(\n",
    "            n_neighbors=self.n_neighbors,\n",
    "            weights=self.weights,\n",
    "            p=self.p,\n",
    "            algorithm=self.algorithm,\n",
    "            leaf_size=self.leaf_size,\n",
    "            n_jobs=self.n_jobs,\n",
    "        )\n",
    "    \n",
    "        self.model_.fit(X_trans, y)\n",
    "    \n",
    "        # 7. training MAE for sanity check\n",
    "        train_pred = self.model_.predict(X_trans)\n",
    "        train_mae = mean_absolute_error(y, train_pred)\n",
    "        if self.knn_verbose:\n",
    "            print(f\"KNN Training MAE: {train_mae:.4f}\")\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Apply the same feature engineering and KNN model to new data.\n",
    "        \"\"\"\n",
    "        if self.fe_ is None or self.model_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted. Call fit(X, y) first.\")\n",
    "    \n",
    "        # 1. transform with FE\n",
    "        X_trans = self.fe_.transform(X)\n",
    "\n",
    "        \"\"\"\n",
    "        # 2. drop non-numeric, same as in fit\n",
    "        non_numeric = X_trans.select_dtypes(exclude=[\"number\", \"bool\"]).columns\n",
    "        if len(non_numeric) > 0 and self.knn_verbose:\n",
    "            print(\"Dropping non-numeric columns for KNN during predict:\", non_numeric.tolist())\n",
    "        X_trans = X_trans.drop(columns=non_numeric)\n",
    "    \n",
    "        # 3. NaN handling, same as in fit\n",
    "        col_means = X_trans.mean(numeric_only=True)\n",
    "        X_trans = X_trans.fillna(col_means)\n",
    "        X_trans = X_trans.fillna(0.0)\n",
    "        \n",
    "    \n",
    "        # 4. align columns to those used during fit\n",
    "        if hasattr(self, \"feature_names_\") and self.feature_names_ is not None:\n",
    "            # drop unexpected cols and add missing ones as 0\n",
    "            X_trans = X_trans.reindex(columns=self.feature_names_, fill_value=0.0)\n",
    "        \"\"\"\n",
    "    \n",
    "        return self.model_.predict(X_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4cc95d82-5f4d-4a90-b54f-32c29a81a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "class NNModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        # FeatureEngineer hyperparams (tunable)\n",
    "        corr_threshold: float = 0.90,\n",
    "        min_fp_freq: float = 0.01,\n",
    "        max_fp_freq: float = 0.90,\n",
    "        fe_verbose: bool = False,\n",
    "        target_col: str = \"Tm\",\n",
    "    ):\n",
    "        # --- FeatureEngineer hyperparams ---\n",
    "        self.corr_threshold = corr_threshold\n",
    "        self.min_fp_freq = min_fp_freq\n",
    "        self.max_fp_freq = max_fp_freq\n",
    "        self.fe_verbose = fe_verbose\n",
    "        self.target_col = target_col\n",
    "\n",
    "        # regularisation\n",
    "        self.l2_reg = regularizers.l2(2e-5)\n",
    "\n",
    "        # will be set in fit\n",
    "        self.fe_ = None\n",
    "        self.model_ = None\n",
    "        self.feature_names_ = None\n",
    "        self.history_ = None\n",
    "\n",
    "    def fit(self, X_train, y_train, X_val=None, y_val=None):\n",
    "        # ----------------- 0. SEEDS ----------------- #\n",
    "        random.seed(SEED)\n",
    "        np.random.seed(SEED)\n",
    "        tf.random.set_seed(SEED)\n",
    "    \n",
    "        # ----------------- 1. HANDLE OPTIONAL VAL SPLIT ----------------- #\n",
    "        if X_val is None or y_val is None:\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                test_size=0.2,\n",
    "                random_state=42,  # or 42, but nice to keep consistent\n",
    "            )\n",
    "    \n",
    "        # ----------------- 2. FEATURE ENGINEERING ----------------- #\n",
    "        # FeatureEngineer with normalization enabled for NN\n",
    "        self.fe_ = FeatureEngineer(\n",
    "            corr_threshold=self.corr_threshold,\n",
    "            min_fp_freq=self.min_fp_freq,\n",
    "            max_fp_freq=self.max_fp_freq,\n",
    "            verbose=self.fe_verbose,\n",
    "            normalize=True,\n",
    "            onehot_cats=True,\n",
    "            cat_cols=[\"Source\"],\n",
    "        )\n",
    "    \n",
    "        # fit FE and transform training / validation data\n",
    "        self.fe_.fit(X_train, target_col=self.target_col)\n",
    "        X_trans = self.fe_.transform(X_train)\n",
    "        X_trans_val = self.fe_.transform(X_val)\n",
    "    \n",
    "        # cast to float32 for TF\n",
    "        X_trans = X_trans.astype(\"float32\")\n",
    "        X_trans_val = X_trans_val.astype(\"float32\")\n",
    "        y_train = np.asarray(y_train, dtype=\"float32\")\n",
    "        y_val = np.asarray(y_val, dtype=\"float32\")\n",
    "    \n",
    "        # store feature names used for training\n",
    "        self.feature_names_ = X_trans.columns.tolist()\n",
    "    \n",
    "        # ----------------- 3. BUILD MODEL ----------------- #\n",
    "        self.model_ = Sequential([\n",
    "            Dense(512, activation=\"relu\", kernel_regularizer=self.l2_reg),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.1),\n",
    "    \n",
    "            Dense(256, activation=\"relu\", kernel_regularizer=self.l2_reg),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.1),\n",
    "    \n",
    "            Dense(256, activation=\"relu\", kernel_regularizer=self.l2_reg),\n",
    "            BatchNormalization(),\n",
    "    \n",
    "            Dense(1),  # output layer for regression\n",
    "        ])\n",
    "    \n",
    "        self.model_.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=\"mae\",\n",
    "            metrics=[\"mae\"],\n",
    "        )\n",
    "    \n",
    "        # ----------------- 4. CALLBACKS ----------------- #\n",
    "        early_stop = EarlyStopping(\n",
    "            monitor=\"val_mae\",\n",
    "            patience=30,\n",
    "            restore_best_weights=True,\n",
    "            verbose=0,\n",
    "        )\n",
    "    \n",
    "        lr_reduce = ReduceLROnPlateau(\n",
    "            monitor=\"val_mae\",\n",
    "            factor=0.8,\n",
    "            patience=10,\n",
    "            min_lr=1e-5,\n",
    "            verbose=0,\n",
    "        )\n",
    "    \n",
    "        # ----------------- 5. TRAIN ----------------- #\n",
    "        self.history_ = self.model_.fit(\n",
    "            X_trans,\n",
    "            y_train,\n",
    "            validation_data=(X_trans_val, y_val),\n",
    "            epochs=500,\n",
    "            batch_size=64,\n",
    "            callbacks=[early_stop, lr_reduce],\n",
    "            verbose=0,\n",
    "            shuffle=True,  \n",
    "        )\n",
    "    \n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.fe_ is None or self.model_ is None:\n",
    "            raise RuntimeError(\"Model is not fitted. Call fit(...) first.\")\n",
    "\n",
    "        # 1. transform with FE\n",
    "        X_trans = self.fe_.transform(X)\n",
    "        X_trans = X_trans.astype(\"float32\")\n",
    "\n",
    "        # 2. predict and flatten to 1D array\n",
    "        return self.model_.predict(X_trans).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7c34ce10-b347-4e42-852e-b231b276032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = combined_df.drop(columns = {\"Tm\"})\n",
    "y = combined_df[\"Tm\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "c3262f28-2487-40af-9ae4-c0e71365b721",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_fe_params = {\n",
    "    \"corr_threshold\": 0.50,\n",
    "    \"min_fp_freq\": 0.18,\n",
    "    \"max_fp_freq\": 0.20,\n",
    "}\n",
    "\n",
    "knn_params = {\n",
    "    \"n_neighbors\": 8,\n",
    "    \"weights\": \"distance\",\n",
    "    \"p\": 1,\n",
    "    \"algorithm\": \"auto\",\n",
    "    \"leaf_size\": 20,\n",
    "}\n",
    "\n",
    "xgb_fe_params = {\n",
    "    \"corr_threshold\": 0.90,\n",
    "    \"min_fp_freq\": 0.01,\n",
    "    \"max_fp_freq\": 0.80,\n",
    "}\n",
    "\n",
    "xgb_params = {\n",
    "    \"max_depth\": 8,\n",
    "    \"n_estimators\": 1400,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"subsample\": 0.7,\n",
    "    \"colsample_bytree\": 1.0,\n",
    "    \"colsample_bylevel\": 0.7,\n",
    "    \"reg_lambda\": 2.0,\n",
    "    \"reg_alpha\": 0.1,\n",
    "    \"gamma\": 0.2,\n",
    "}\n",
    "\n",
    "cb_fe_params = {\n",
    "    \"corr_threshold\": 0.90,\n",
    "    \"min_fp_freq\": 0.01,\n",
    "    \"max_fp_freq\": 0.80,\n",
    "}\n",
    "\n",
    "cb_params = {\n",
    "    \"depth\": 8,\n",
    "    \"n_estimators\": 1600,\n",
    "    \"l2_leaf_reg\": 18,\n",
    "    \"learning_rate\": 0.075,\n",
    "    \"bagging_temperature\": 0.30,\n",
    "    \"random_strength\": 0.75,\n",
    "    \"grow_policy\": \"SymmetricTree\",\n",
    "    \"rsm\": 0.45,\n",
    "    \"min_data_in_leaf\": 18,\n",
    "    \"leaf_estimation_iterations\": 5,\n",
    "    \"loss_function\": \"MAE\",\n",
    "}\n",
    "\n",
    "nn_fe_params = {\n",
    "    \"corr_threshold\": 0.95,\n",
    "    \"min_fp_freq\": 0.001,\n",
    "    \"max_fp_freq\": 0.999,\n",
    "}\n",
    "    \n",
    "best_cat_model = CatBoostFEModel(**cb_params, **cb_fe_params)\n",
    "best_xgb_model = XGBModel(**xgb_params, **xgb_fe_params)\n",
    "best_knn_model = KNNModel(**knn_params, **knn_fe_params)\n",
    "best_nn_model = NNModel(**nn_fe_params)\n",
    "\n",
    "base_models = {\n",
    "    \"best_cat_model\": best_cat_model,\n",
    "    \"best_xgb_model\": best_xgb_model,\n",
    "    \"best_knn_model\": best_knn_model,\n",
    "    \"best_nn_model\": best_nn_model,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "248906ef-9eb1-4b0e-b40b-c22ca6c63f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 51: ReduceLROnPlateau reducing learning rate to 0.000800000037997961.\n",
      "\n",
      "Epoch 66: ReduceLROnPlateau reducing learning rate to 0.0006400000303983689.\n",
      "\n",
      "Epoch 76: ReduceLROnPlateau reducing learning rate to 0.0005120000336319208.\n",
      "\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00040960004553198815.\n",
      "\n",
      "Epoch 107: ReduceLROnPlateau reducing learning rate to 0.00032768002711236477.\n",
      "\n",
      "Epoch 117: ReduceLROnPlateau reducing learning rate to 0.0002621440216898918.\n",
      "Epoch 117: early stopping\n",
      "Restoring model weights from the end of the best epoch: 87.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.NNModel at 0x1c9ae14d010>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cat_model.fit(X_train, y_train)\n",
    "best_xgb_model.fit(X_train, y_train)\n",
    "best_knn_model.fit(X_train, y_train)\n",
    "best_nn_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "54cbfcb4-68e3-4337-8819-da630936a253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m106/106\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n",
      " CatBoost MAE: 25.74567849456645\n",
      " XGB MAE: 26.111841918149125\n",
      " KNN MAE: 36.22784764527904\n",
      " NN MAE: 26.620024754569187\n"
     ]
    }
   ],
   "source": [
    "y_cat = best_cat_model.predict(X_val)\n",
    "y_xgb = best_xgb_model.predict(X_val)\n",
    "y_knn = best_knn_model.predict(X_val)\n",
    "y_nn = best_nn_model.predict(X_val)\n",
    "\n",
    "print(f\" CatBoost MAE: {mean_absolute_error(y_cat, y_val)}\")\n",
    "print(f\" XGB MAE: {mean_absolute_error(y_xgb, y_val)}\")\n",
    "print(f\" KNN MAE: {mean_absolute_error(y_knn, y_val)}\")\n",
    "print(f\" NN MAE: {mean_absolute_error(y_nn, y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ffc36efb-250a-49dd-8335-1c19cc78239c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "  Training base model 'best_cat_model' on fold 1...\n",
      "    best_cat_model fold MAE: 29.315\n",
      "  Training base model 'best_xgb_model' on fold 1...\n",
      "    best_xgb_model fold MAE: 29.632\n",
      "  Training base model 'best_knn_model' on fold 1...\n",
      "    best_knn_model fold MAE: 37.342\n",
      "  Training base model 'best_nn_model' on fold 1...\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "    best_nn_model fold MAE: 31.527\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "  Training base model 'best_cat_model' on fold 2...\n",
      "    best_cat_model fold MAE: 28.226\n",
      "  Training base model 'best_xgb_model' on fold 2...\n",
      "    best_xgb_model fold MAE: 28.585\n",
      "  Training base model 'best_knn_model' on fold 2...\n",
      "    best_knn_model fold MAE: 37.764\n",
      "  Training base model 'best_nn_model' on fold 2...\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "    best_nn_model fold MAE: 30.464\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "  Training base model 'best_cat_model' on fold 3...\n",
      "    best_cat_model fold MAE: 29.060\n",
      "  Training base model 'best_xgb_model' on fold 3...\n",
      "    best_xgb_model fold MAE: 28.658\n",
      "  Training base model 'best_knn_model' on fold 3...\n",
      "    best_knn_model fold MAE: 36.271\n",
      "  Training base model 'best_nn_model' on fold 3...\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "    best_nn_model fold MAE: 30.123\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "  Training base model 'best_cat_model' on fold 4...\n",
      "    best_cat_model fold MAE: 28.054\n",
      "  Training base model 'best_xgb_model' on fold 4...\n",
      "    best_xgb_model fold MAE: 29.258\n",
      "  Training base model 'best_knn_model' on fold 4...\n",
      "    best_knn_model fold MAE: 38.974\n",
      "  Training base model 'best_nn_model' on fold 4...\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "    best_nn_model fold MAE: 30.579\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "  Training base model 'best_cat_model' on fold 5...\n",
      "    best_cat_model fold MAE: 28.476\n",
      "  Training base model 'best_xgb_model' on fold 5...\n",
      "    best_xgb_model fold MAE: 28.219\n",
      "  Training base model 'best_knn_model' on fold 5...\n",
      "    best_knn_model fold MAE: 38.993\n",
      "  Training base model 'best_nn_model' on fold 5...\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step \n",
      "    best_nn_model fold MAE: 30.152\n",
      "X_meta shape: (2662, 4)\n",
      "\n",
      "Fitting best_cat_model on full training data...\n",
      "\n",
      "Fitting best_xgb_model on full training data...\n",
      "\n",
      "Fitting best_knn_model on full training data...\n",
      "\n",
      "Fitting best_nn_model on full training data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "import copy\n",
    "\n",
    "model_names = list(base_models.keys())\n",
    "n_models = len(model_names)\n",
    "\n",
    "# full training data\n",
    "X_train_full = train_df.drop(columns=[\"Tm\"])\n",
    "y_train_full = train_df[\"Tm\"].values\n",
    "\n",
    "n_samples = X_train_full.shape[0]\n",
    "\n",
    "# OOF prediction matrix\n",
    "X_meta = np.zeros((n_samples, n_models), dtype=float)\n",
    "y_meta = y_train_full.copy()  # same targets\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for fold, (tr_idx, val_idx) in enumerate(kf.split(X_train_full)):\n",
    "    print(f\"\\n=== Fold {fold+1}/5 ===\")\n",
    "    X_tr, X_val = X_train_full.iloc[tr_idx], X_train_full.iloc[val_idx]\n",
    "    y_tr, y_val = y_train_full[tr_idx], y_train_full[val_idx]\n",
    "\n",
    "    for j, (name, template_model) in enumerate(base_models.items()):\n",
    "        print(f\"  Training base model '{name}' on fold {fold+1}...\")\n",
    "        # deep copy to keep hyperparams but reset fit state\n",
    "        model = copy.deepcopy(template_model)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        X_meta[val_idx, j] = y_val_pred\n",
    "\n",
    "        fold_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "        print(f\"    {name} fold MAE: {fold_mae:.3f}\")\n",
    "\n",
    "print(\"X_meta shape:\", X_meta.shape)  # (n_samples, n_models)\n",
    "\n",
    "# ----------------- 3. FIT RIDGE STACKER ON META-DATA ----------------- #\n",
    "\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_meta, y_meta)\n",
    "\n",
    "# ----------------- 4. REFIT BASE MODELS ON FULL TRAINING DATA -------- #\n",
    "\n",
    "fitted_base_models = {}\n",
    "for name, template_model in base_models.items():\n",
    "    print(f\"\\nFitting {name} on full training data...\")\n",
    "    model = copy.deepcopy(template_model)\n",
    "    model.fit(X_train_full, y_train_full)\n",
    "    fitted_base_models[name] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dec37759-0feb-4a4b-9fef-f0bb6c34ba2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n"
     ]
    }
   ],
   "source": [
    "# ----------------- 5. BUILD META-FEATURES FOR TEST ------------------- #\n",
    "\n",
    "X_test = test_df  # Kaggle test, no Tm\n",
    "\n",
    "N_test = X_test.shape[0]\n",
    "test_meta_features = np.zeros((N_test, n_models), dtype=float)\n",
    "\n",
    "for j, (name, model) in enumerate(fitted_base_models.items()):\n",
    "    test_meta_features[:, j] = model.predict(X_test)\n",
    "\n",
    "# ----------------- 6. STACKED PREDICTIONS FOR TEST ------------------- #\n",
    "\n",
    "y_test_ens = ridge.predict(test_meta_features)\n",
    "\n",
    "# e.g. build submission\n",
    "sub = pd.DataFrame({\n",
    "    \"id\": test_df.index,  \n",
    "    \"Tm\": y_test_ens,\n",
    "})\n",
    "\n",
    "sub = sub.set_index(\"id\")\n",
    "sub.to_csv(\"submission_stacked_ridge.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a8b69dd8-03ea-47fc-a118-fb09f61e10d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>386.279463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>324.856241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>512.100565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>659.891731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>235.741228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3312</th>\n",
       "      <td>331.169472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3313</th>\n",
       "      <td>350.342216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3314</th>\n",
       "      <td>408.264941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3321</th>\n",
       "      <td>318.630189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3327</th>\n",
       "      <td>357.962934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>666 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Tm\n",
       "id              \n",
       "2     386.279463\n",
       "4     324.856241\n",
       "9     512.100565\n",
       "18    659.891731\n",
       "19    235.741228\n",
       "...          ...\n",
       "3312  331.169472\n",
       "3313  350.342216\n",
       "3314  408.264941\n",
       "3321  318.630189\n",
       "3327  357.962934\n",
       "\n",
       "[666 rows x 1 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "86529f8b-2737-4e53-bcec-96984d70c768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: -3.109366193548169\n",
      "Coefs: {'best_cat_model': np.float64(0.27643706631411186), 'best_xgb_model': np.float64(0.5935871975098954), 'best_knn_model': np.float64(-0.01732792633287298), 'best_nn_model': np.float64(0.1689990040558685)}\n"
     ]
    }
   ],
   "source": [
    "print(\"Intercept:\", ridge.intercept_)\n",
    "print(\"Coefs:\", dict(zip(model_names, ridge.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c40ce0d2-6f84-4772-975c-33d3a714b05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights = ridge.coef_\n",
    "plt.bar(model_names, weights)\n",
    "plt.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Ridge Stacking Weights\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a12836-7940-4fff-9b0a-88ddfa56a9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
